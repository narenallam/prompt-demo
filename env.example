# LLM Provider Configuration
# Copy this file to .env and fill in your API keys

# ============================================================================
# PROVIDER SELECTION
# ============================================================================
# Set provider to one of: ollama, openai, gemini
PROVIDER=ollama

# ============================================================================
# OLLAMA CONFIGURATION
# ============================================================================
# Model name (e.g., deepseek-r1-32b:latest, llama3.2, mistral, etc.)
OLLAMA_MODEL=deepseek-r1-32b:latest
# Base URL for Ollama API
OLLAMA_BASE_URL=http://localhost:11434
# Maximum tokens to generate (optional, leave empty for no limit)
# OLLAMA_NUM_PREDICT=1000

# ============================================================================
# OPENAI CONFIGURATION
# ============================================================================
# Your OpenAI API key
OPENAI_API_KEY=sk-your-openai-api-key-here
# Model name (e.g., gpt-3.5-turbo, gpt-4, gpt-4o-mini, gpt-4-turbo)
OPENAI_MODEL=gpt-3.5-turbo
# Custom base URL (optional, for OpenAI-compatible APIs)
# OPENAI_BASE_URL=https://api.openai.com/v1
# Maximum tokens to generate (optional)
# OPENAI_MAX_TOKENS=1000

# ============================================================================
# GEMINI CONFIGURATION
# ============================================================================
# Your Google API key for Gemini
GOOGLE_API_KEY=your-google-api-key-here
# Model name (e.g., gemini-pro, gemini-1.5-pro, gemini-1.5-flash)
GEMINI_MODEL=gemini-pro
# Maximum output tokens (optional)
# GEMINI_MAX_OUTPUT_TOKENS=1000

